{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00mFKyp7YSMQ"
      },
      "source": [
        "# Assignment 1 - Code Example - Part B\n",
        "\n",
        "This achieves an accuracy of 93.26% on test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9mnvSFGYSMS"
      },
      "outputs": [],
      "source": [
        "# provide some basic operators like matrix multiplication\n",
        "import cupy as cp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd9w7EgEYSMS"
      },
      "source": [
        "## Some Useful Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X14d4YYaYSMS"
      },
      "outputs": [],
      "source": [
        "# base class\n",
        "class Module:\n",
        "    @property\n",
        "    def params(self): # trainable parameters\n",
        "        return []\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward( *args, **kwargs)\n",
        "\n",
        "# sequential module\n",
        "class Sequential(Module, list):\n",
        "    def __init__(self, *module_lst):\n",
        "        super().__init__(module_lst)\n",
        "\n",
        "    @property\n",
        "    def params(self):\n",
        "        return sum([m.params for m in self], []) # concat all params\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x\n",
        "        for module in self:\n",
        "            y = module(y)\n",
        "        return y\n",
        "\n",
        "    def backward(self, dy):\n",
        "        dx = dy\n",
        "        for module in self[::-1]:\n",
        "            dx = module.backward(dx)\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEDpu1ICYSMT"
      },
      "source": [
        "## Softplus\n",
        "\n",
        "This implements the [Softplus](https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html) function.\n",
        "\n",
        "$y = \\frac{1}{\\beta} \\ln(1+e^{\\beta x})$\n",
        "\n",
        "$y' = \\frac{1}{1+e^{-\\beta x}}$\n",
        "\n",
        "Default: $\\beta=1$\n",
        "\n",
        "$e^{\\beta x}$ might be too large and unstable; so we use linear function to approximate it when $\\beta x$ is above the threshold $20$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDjGe9P6YSMT"
      },
      "outputs": [],
      "source": [
        "class Softplus(Module):\n",
        "    def __init__(self, beta=1.0, threshold=20.0):\n",
        "        assert beta > 0 and threshold > 0\n",
        "        self.beta = beta\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.beta_x = self.beta * x # save the input for backward use\n",
        "        y = cp.log(1 + cp.exp(self.beta_x)) / self.beta\n",
        "        y_relu = cp.where(x > 0, x, 0)\n",
        "        return cp.where(x < self.threshold, y, y_relu)\n",
        "\n",
        "    def backward(self, dy):\n",
        "        grad = 1 / (1 + cp.exp(-self.beta_x))\n",
        "        grad_relu = cp.where(self.beta_x > 0, 1, 0)\n",
        "        return dy * cp.where(self.beta_x < self.threshold, grad, grad_relu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfxoduz-YSMT"
      },
      "source": [
        "## LinearNoBias\n",
        "\n",
        "This implements the [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer but without the bias term.\n",
        "\n",
        "$y = x W^T$\n",
        "\n",
        "$dy/dx = W$\n",
        "\n",
        "$dy/dW = x$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYgnGK-UYSMT"
      },
      "outputs": [],
      "source": [
        "class LinearNoBias(Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        self.weight = (cp.random.rand(out_features, in_features) * 2 - 1) / in_features ** 0.5\n",
        "        self.weight_grad = cp.zeros_like(self.weight)\n",
        "\n",
        "    @property\n",
        "    def params(self):\n",
        "        return [dict(val=self.weight, grad=self.weight_grad)]\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return x @ self.weight.T\n",
        "\n",
        "    def backward(self, dy):\n",
        "        self.weight_grad[:] = dy.T @ self.x\n",
        "        return dy @ self.weight\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwCJllw9YSMT"
      },
      "source": [
        "## CrossEntropyLoss\n",
        "\n",
        "This implements the [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vr6ZfJ30YSMU"
      },
      "outputs": [],
      "source": [
        "def onehot(x, num_classes=10):\n",
        "    y = cp.zeros([len(x), num_classes])\n",
        "    y[cp.arange(len(x)), x] = 1\n",
        "    return y\n",
        "\n",
        "\n",
        "class CrossEntropyLoss(Module):\n",
        "    def forward(self, x_logit, x_target):\n",
        "        self.x_logit = x_logit\n",
        "        self.x_target = x_target\n",
        "\n",
        "        # softmax with numerical stability\n",
        "        x_logit_sub = cp.exp(x_logit - cp.max(x_logit, axis=1, keepdims=True))\n",
        "        x_softmax = x_logit_sub / cp.sum(x_logit_sub, axis=1, keepdims=True)\n",
        "        x_softmax = cp.clip(x_softmax, a_min=1e-15, a_max=None)  # Corrected line\n",
        "        self.x_softmax = x_softmax\n",
        "\n",
        "        # loss calculation\n",
        "        loss_x = -cp.log(x_softmax)[cp.arange(len(x_target)), x_target]\n",
        "        return loss_x.mean()\n",
        "\n",
        "    def backward(self, dy):\n",
        "        return dy * (self.x_softmax - onehot(self.x_target)) / len(self.x_logit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baA5yw51YSMU"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1BsGkRPYSMU"
      },
      "outputs": [],
      "source": [
        "class Optim:\n",
        "    def __init__(self, params, lr=0.01):\n",
        "        self.params = params\n",
        "        self.lr = lr\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for idx in range(len(self.params)):\n",
        "            self.params[idx][\"grad\"][:] = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YIWzDvOYSMU"
      },
      "outputs": [],
      "source": [
        "class SGD(Optim):\n",
        "    def step(self):\n",
        "        for idx in range(len(self.params)):\n",
        "            self.params[idx][\"val\"] -= self.lr * self.params[idx][\"grad\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sigmoid"
      ],
      "metadata": {
        "id": "2X9-2pvPgMTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid(Module):\n",
        "    def forward(self, x):\n",
        "        self.sigmoid = 1 / (1 + cp.exp(-x))\n",
        "        return self.sigmoid\n",
        "\n",
        "    def backward(self, dy):\n",
        "        return dy * self.sigmoid * (1 - self.sigmoid)"
      ],
      "metadata": {
        "id": "AfefbTkpgLMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LeakyReLU Activation"
      ],
      "metadata": {
        "id": "m_zl6NJZgUy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeakyReLU(Module):\n",
        "    def __init__(self, negative_slope=0.01):\n",
        "        self.negative_slope = negative_slope\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return cp.where(x > 0, x, self.negative_slope * x)\n",
        "\n",
        "    def backward(self, dy):\n",
        "        return dy * cp.where(self.x > 0, 1, self.negative_slope)"
      ],
      "metadata": {
        "id": "aKCR5WvugWM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SELU Activation"
      ],
      "metadata": {
        "id": "NJk4Y0rKgU8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SELU(Module):\n",
        "    def __init__(self):\n",
        "        self.alpha = 1.6732632423543772848170429916717\n",
        "        self.scale = 1.0507009873554804934193349852946\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        mask = x > 0\n",
        "        return self.scale * (x * mask + self.alpha * (cp.exp(x * ~mask) - 1) * ~mask)\n",
        "\n",
        "    def backward(self, dy):\n",
        "        mask = self.x > 0\n",
        "        grad = cp.where(mask, self.scale, self.scale * self.alpha * cp.exp(self.x))\n",
        "        return dy * grad"
      ],
      "metadata": {
        "id": "BGXKmBOvgVHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Linear Layer"
      ],
      "metadata": {
        "id": "mRlisY7zgl98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(Module):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = cp.random.randn(out_features, in_features) * cp.sqrt(2. / in_features)\n",
        "        self.weight_grad = cp.zeros_like(self.weight)\n",
        "        if bias:\n",
        "            self.bias = cp.zeros(out_features)\n",
        "            self.bias_grad = cp.zeros_like(self.bias)\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    @property\n",
        "    def params(self):\n",
        "        params = [{'val': self.weight, 'grad': self.weight_grad}]\n",
        "        if self.bias is not None:\n",
        "            params.append({'val': self.bias, 'grad': self.bias_grad})\n",
        "        return params\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        output = x @ self.weight.T\n",
        "        if self.bias is not None:\n",
        "            output += self.bias\n",
        "        return output\n",
        "\n",
        "    def backward(self, dy):\n",
        "        self.weight_grad[:] = dy.T @ self.x\n",
        "        if self.bias is not None:\n",
        "            self.bias_grad[:] = dy.sum(axis=0)\n",
        "        return dy @ self.weight"
      ],
      "metadata": {
        "id": "zlBBrGKsgnKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conv2d Layer"
      ],
      "metadata": {
        "id": "yzFlO1MfgpMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cupy.lib.stride_tricks import as_strided\n",
        "\n",
        "class Conv2d(Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, bias=True):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
        "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
        "        self.bias = bias\n",
        "\n",
        "        kernel_h, kernel_w = self.kernel_size\n",
        "        scale = cp.sqrt(2. / (in_channels * kernel_h * kernel_w))  # He initialization\n",
        "        self.weights = cp.random.normal(0, scale, size=(out_channels, in_channels, kernel_h, kernel_w))\n",
        "        self.weights_grad = cp.zeros_like(self.weights)\n",
        "\n",
        "        if self.bias:\n",
        "            self.bias_weights = cp.zeros(out_channels)\n",
        "            self.bias_grad = cp.zeros_like(self.bias_weights)\n",
        "        else:\n",
        "            self.bias_weights = None\n",
        "            self.bias_grad = None\n",
        "\n",
        "        self.x = None\n",
        "        self.H_out = None\n",
        "        self.W_out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        N, C_in, H_in, W_in = x.shape\n",
        "        kernel_h, kernel_w = self.kernel_size\n",
        "        stride_h, stride_w = self.stride\n",
        "\n",
        "        H_out = (H_in - kernel_h) // stride_h + 1\n",
        "        W_out = (W_in - kernel_w) // stride_w + 1\n",
        "        self.H_out = H_out\n",
        "        self.W_out = W_out\n",
        "\n",
        "        x_col = self.im2col(x, kernel_h, kernel_w, stride_h, stride_w, H_out, W_out)\n",
        "\n",
        "        weight_matrix = self.weights.reshape(self.out_channels, -1)\n",
        "        output = weight_matrix @ x_col.T\n",
        "        output = output.reshape(self.out_channels, N, H_out, W_out).transpose(1, 0, 2, 3)\n",
        "\n",
        "        if self.bias:\n",
        "            output += self.bias_weights.reshape(1, -1, 1, 1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        x = self.x\n",
        "        N, C_in, H_in, W_in = x.shape\n",
        "        C_out = self.out_channels\n",
        "        kernel_h, kernel_w = self.kernel_size\n",
        "        stride_h, stride_w = self.stride\n",
        "        H_out = self.H_out\n",
        "        W_out = self.W_out\n",
        "\n",
        "        x_col = self.im2col(x, kernel_h, kernel_w, stride_h, stride_w, H_out, W_out)\n",
        "\n",
        "        grad_output_reshaped = grad_output.transpose(1, 0, 2, 3).reshape(C_out, -1)\n",
        "\n",
        "        self.weights_grad[...] = (grad_output_reshaped @ x_col).reshape(self.weights.shape)\n",
        "\n",
        "        if self.bias:\n",
        "            self.bias_grad[...] = grad_output.sum(axis=(0, 2, 3))\n",
        "\n",
        "        weight_matrix = self.weights.reshape(C_out, C_in * kernel_h * kernel_w)\n",
        "        dx_col = weight_matrix.T @ grad_output_reshaped  # (C_in * K * K, N * H_out * W_out)\n",
        "\n",
        "        dx_col = dx_col.T.reshape(N, H_out, W_out, C_in, kernel_h, kernel_w)\n",
        "        dx_col = dx_col.transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "        dx = cp.zeros_like(x)\n",
        "        dx_view = as_strided(dx,\n",
        "            shape=(N, C_in, kernel_h, kernel_w, H_out, W_out),\n",
        "            strides=(\n",
        "                dx.strides[0],\n",
        "                dx.strides[1],\n",
        "                dx.strides[2],\n",
        "                dx.strides[3],\n",
        "                stride_h * dx.strides[2],\n",
        "                stride_w * dx.strides[3],\n",
        "            )\n",
        "        )\n",
        "        dx_view += dx_col\n",
        "\n",
        "        return dx\n",
        "\n",
        "    def im2col(self, x, kernel_h, kernel_w, stride_h, stride_w, H_out, W_out):\n",
        "        N, C_in, H_in, W_in = x.shape\n",
        "        shape = (N, C_in, kernel_h, kernel_w, H_out, W_out)\n",
        "        strides = (\n",
        "            x.strides[0],\n",
        "            x.strides[1],\n",
        "            x.strides[2],\n",
        "            x.strides[3],\n",
        "            stride_h * x.strides[2],\n",
        "            stride_w * x.strides[3],\n",
        "        )\n",
        "        x_col = as_strided(x, shape=shape, strides=strides)\n",
        "        x_col = x_col.transpose(0, 4, 5, 1, 2, 3).reshape(N * H_out * W_out, -1)\n",
        "        return x_col\n",
        "\n",
        "    @property\n",
        "    def params(self):\n",
        "        if self.bias:\n",
        "            return [\n",
        "                dict(val=self.weights, grad=self.weights_grad),\n",
        "                dict(val=self.bias_weights, grad=self.bias_grad)\n",
        "            ]\n",
        "        else:\n",
        "            return [dict(val=self.weights, grad=self.weights_grad)]"
      ],
      "metadata": {
        "id": "XyeN9zd_gpXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dropout Layer"
      ],
      "metadata": {
        "id": "j95AwHpHgpem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dropout(Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        if training:\n",
        "            self.mask = (cp.random.rand(*x.shape) > self.p) / (1 - self.p)\n",
        "            return x * self.mask\n",
        "        return x\n",
        "\n",
        "    def backward(self, dy):\n",
        "        return dy * self.mask"
      ],
      "metadata": {
        "id": "YIYCtmvBgplH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BatchNorm2d Layer"
      ],
      "metadata": {
        "id": "ff97Mhzrgppc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm2d(Module):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        self.gamma = cp.ones(num_features)\n",
        "        self.beta = cp.zeros(num_features)\n",
        "        self.running_mean = cp.zeros(num_features)\n",
        "        self.running_var = cp.ones(num_features)\n",
        "        self.gamma_grad = cp.zeros_like(self.gamma)\n",
        "        self.beta_grad = cp.zeros_like(self.beta)\n",
        "\n",
        "    @property\n",
        "    def params(self):\n",
        "        return [{'val': self.gamma, 'grad': self.gamma_grad},\n",
        "                {'val': self.beta, 'grad': self.beta_grad}]\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        if training:\n",
        "            mean = x.mean(axis=(0, 2, 3), keepdims=True)\n",
        "            var = x.var(axis=(0, 2, 3), keepdims=True)\n",
        "            self.x_hat = (x - mean) / cp.sqrt(var + self.eps)\n",
        "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.squeeze()\n",
        "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var.squeeze()\n",
        "        else:\n",
        "            self.x_hat = (x - self.running_mean[None, :, None, None]) / cp.sqrt(self.running_var[None, :, None, None] + self.eps)\n",
        "        return self.gamma[None, :, None, None] * self.x_hat + self.beta[None, :, None, None]\n",
        "\n",
        "    def backward(self, dy):\n",
        "        N, C, H, W = dy.shape\n",
        "        dx_hat = dy * self.gamma[None, :, None, None]\n",
        "        dvar = (dx_hat * self.x_hat).sum(axis=(0, 2, 3), keepdims=True) * (-0.5) * (self.x_hat ** 2 + self.eps) ** (-1.5)\n",
        "        dmean = (dx_hat * (-1 / cp.sqrt(self.x_hat ** 2 + self.eps))).sum(axis=(0, 2, 3), keepdims=True) + dvar * (-2 * self.x_hat).mean(axis=(0, 2, 3), keepdims=True)\n",
        "        dx = (dx_hat / cp.sqrt(self.x_hat ** 2 + self.eps)) + dvar * 2 * self.x_hat / (N * H * W) + dmean / (N * H * W)\n",
        "        self.gamma_grad[:] = (dy * self.x_hat).sum(axis=(0, 2, 3))\n",
        "        self.beta_grad[:] = dy.sum(axis=(0, 2, 3))\n",
        "        return dx"
      ],
      "metadata": {
        "id": "JipeWje-gpuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Focal Loss"
      ],
      "metadata": {
        "id": "6w93cvxsg2rL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2):\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        x_exp = cp.exp(x - cp.max(x, axis=1, keepdims=True))\n",
        "        softmax = x_exp / x_exp.sum(axis=1, keepdims=True)\n",
        "        self.pt = softmax[cp.arange(len(target)), target]\n",
        "        loss = -self.alpha * (1 - self.pt) ** self.gamma * cp.log(self.pt)\n",
        "        return loss.mean()\n",
        "\n",
        "    def backward(self, dy):\n",
        "        batch_size = len(self.pt)\n",
        "        pt = self.pt\n",
        "        grad = cp.zeros_like(self.pt)\n",
        "        grad_term = self.alpha * (1 - pt) ** self.gamma * (self.gamma * pt * cp.log(pt) + pt - 1)\n",
        "        grad = grad_term * (1 - self.pt) / batch_size\n",
        "        return grad"
      ],
      "metadata": {
        "id": "OiRVAuWPg22r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SGD with Momentum"
      ],
      "metadata": {
        "id": "-orT1IIGg28f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD(Optim):\n",
        "    def __init__(self, params, lr=0.01, momentum=0.9):\n",
        "        super().__init__(params, lr)\n",
        "        self.momentum = momentum\n",
        "        self.velocities = [cp.zeros_like(p['val']) for p in params]\n",
        "\n",
        "    def step(self):\n",
        "        for i, param in enumerate(self.params):\n",
        "            self.velocities[i] = self.momentum * self.velocities[i] + param['grad']\n",
        "            param['val'] -= self.lr * self.velocities[i]"
      ],
      "metadata": {
        "id": "mWdV-ArQg3Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Adam Optimizer"
      ],
      "metadata": {
        "id": "eZr0U05Lg3Hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam(Optim):\n",
        "    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        super().__init__(params, lr)\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.m = [cp.zeros_like(p['val']) for p in params]\n",
        "        self.v = [cp.zeros_like(p['val']) for p in params]\n",
        "        self.t = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.t += 1\n",
        "        for i, param in enumerate(self.params):\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * param['grad']\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (param['grad'] ** 2)\n",
        "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
        "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "            param['val'] -= self.lr * m_hat / (cp.sqrt(v_hat) + self.eps)"
      ],
      "metadata": {
        "id": "pWp-t6Bpg3NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.input_shape = x.shape\n",
        "        return x.reshape(x.shape[0], -1)  # Flatten all dimensions except the batch dimension\n",
        "\n",
        "    def backward(self, dy):\n",
        "        return dy.reshape(self.input_shape)  # Reshape back to the original shape"
      ],
      "metadata": {
        "id": "YLs4rJPujLsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJeZ_hfKYSMU"
      },
      "source": [
        "## Your Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uncMWsEnYSMU"
      },
      "outputs": [],
      "source": [
        "net = Sequential(\n",
        "    # Feature extractor\n",
        "    Conv2d(1, 32, kernel_size=3), # 28 -> 26\n",
        "    BatchNorm2d(32),\n",
        "    LeakyReLU(),\n",
        "\n",
        "    Conv2d(32, 64, kernel_size=3),\n",
        "    BatchNorm2d(64),\n",
        "    LeakyReLU(),\n",
        "\n",
        "    Conv2d(64, 128, kernel_size=3),\n",
        "    BatchNorm2d(128),\n",
        "    LeakyReLU(),\n",
        "\n",
        "    # Classifier\n",
        "    Flatten(),\n",
        "    Linear(22*22*128, 128),\n",
        "    LeakyReLU(),\n",
        "    Dropout(0.3),\n",
        "    Linear(128, 10)\n",
        ")\n",
        "\n",
        "loss_fn = CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ0Lw5byYSMU"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6nWqtyFYSMU"
      },
      "outputs": [],
      "source": [
        "# torch and torchvision provide some very handy utilities for dataset loading\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as tv_datasets\n",
        "import torchvision.transforms as tv_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5yC2WDjYSMU"
      },
      "outputs": [],
      "source": [
        "# some experimental setup\n",
        "num_epochs = 5\n",
        "batch_size = 128\n",
        "num_workers = 2\n",
        "print_every = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27bY_Py6YSMU"
      },
      "outputs": [],
      "source": [
        "# prepare datasets\n",
        "dataset, loader = {}, {}\n",
        "for data_type in (\"train\", \"test\"):\n",
        "    is_train = data_type==\"train\"\n",
        "    dataset[data_type] = tv_datasets.MNIST(\n",
        "        root=\"./data\", train=is_train, download=True,\n",
        "        transform=tv_transforms.Compose([ # preprocessing pipeline for input images\n",
        "            tv_transforms.ToTensor(),\n",
        "            tv_transforms.Normalize((0.1307,), (0.3081,)),\n",
        "    ]))\n",
        "    loader[data_type] = DataLoader(\n",
        "        dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "LPutmEacYSMU",
        "outputId": "ab8cc8a1-1170-44a6-9002-ab47b492b9d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch=  1, iter=  100] loss: 0.428\n",
            "[epoch=  1, iter=  200] loss: 0.233\n",
            "[epoch=  1, iter=  300] loss: 0.194\n",
            "[epoch=  1, iter=  400] loss: 0.154\n",
            "[epoch=  2, iter=  100] loss: 0.110\n",
            "[epoch=  2, iter=  200] loss: 0.105\n",
            "[epoch=  2, iter=  300] loss: 0.094\n",
            "[epoch=  2, iter=  400] loss: 0.094\n",
            "[epoch=  3, iter=  100] loss: 0.072\n",
            "[epoch=  3, iter=  200] loss: 0.070\n",
            "[epoch=  3, iter=  300] loss: 0.073\n",
            "[epoch=  3, iter=  400] loss: 0.065\n",
            "[epoch=  4, iter=  100] loss: 0.050\n",
            "[epoch=  4, iter=  200] loss: 0.052\n",
            "[epoch=  4, iter=  300] loss: 0.049\n",
            "[epoch=  4, iter=  400] loss: 0.050\n",
            "[epoch=  5, iter=  100] loss: 0.040\n",
            "[epoch=  5, iter=  200] loss: 0.038\n",
            "[epoch=  5, iter=  300] loss: 0.039\n",
            "[epoch=  5, iter=  400] loss: 0.041\n",
            "[epoch=  6, iter=  100] loss: 0.034\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-c2bab828b072>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "optimizer = Adam(\n",
        "    params=net.params,\n",
        "    lr=1e-4,\n",
        "    beta1=0.9,\n",
        "    beta2=0.999,\n",
        "    eps=1e-8\n",
        ")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
        "        img, target = cp.asarray(img.numpy()), cp.asarray(target.numpy())\n",
        "        # img = img.reshape(-1, 784)\n",
        "\n",
        "        loss = loss_fn(net(img), target)\n",
        "\n",
        "        net.backward(loss_fn.backward(loss))\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % print_every == print_every - 1:\n",
        "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"Finished Training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to be stopped because the GPU quota is running out"
      ],
      "metadata": {
        "id": "sAnQuJH1pI6d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvPZ7FAIYSMV"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfQK58lTYSMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a984ea65-c3a6-40a7-f1e5-da99aa81d959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 97.59%\n"
          ]
        }
      ],
      "source": [
        "# for each test image\n",
        "correct, total = 0, 0\n",
        "for img, target in loader[\"test\"]:\n",
        "    img, target = cp.asarray(img.numpy()), cp.asarray(target.numpy())\n",
        "    # img = img.reshape(-1, 784)\n",
        "\n",
        "    # make prediction\n",
        "    pred = net(img)\n",
        "\n",
        "    # accumulate\n",
        "    total += len(target)\n",
        "    correct += (cp.argmax(pred, axis=1) == target).sum()\n",
        "\n",
        "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}